{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0fdad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import spacy\n",
    "from utils import build_phrase_vocab\n",
    "from preprocessing import preprocess_with_phrases  # fixed typo: pre_processing -> preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474de27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum sequence length and minimum sentence length for training\n",
    "sequence_len = 128  # Maximum number of characters in a sentence\n",
    "min_len = 5         # Minimum number of characters in a sentence\n",
    "\n",
    "# Provide your English-Hindi sentence pairs for training\n",
    "en_texts = [\"Hello world.\", \"How are you?\"]  # List of English sentences\n",
    "hi_texts = [\"नमस्ते दुनिया।\", \"आप कैसे हैं?\"]  # List of corresponding Hindi sentences\n",
    "\n",
    "# Preprocess the data: clean, filter by length, and extract phrase tags from English\n",
    "en_proc, hi_proc, phrase_tags = preprocess_with_phrases(en_texts, hi_texts, min_len, sequence_len)\n",
    "\n",
    "# Build a vocabulary for phrase tags\n",
    "phrase2idx = build_phrase_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1332ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharPhraseDataset(Dataset):\n",
    "    def __init__(self, x, y, phrases, sequence_len, ch2i, phrase2idx):\n",
    "        # Store the processed sentences, phrase tags, and vocabularies\n",
    "        self.x, self.y, self.phrases = x, y, phrases\n",
    "        self.sequence_len = sequence_len\n",
    "        self.ch2i = ch2i\n",
    "        self.phrase2idx = phrase2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of sentence pairs in the dataset\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert characters to indices for both English and Hindi sentences\n",
    "        x = [self.ch2i.get(c, 0) for c in self.x[idx]]\n",
    "        y = [self.ch2i.get(c, 0) for c in self.y[idx]]\n",
    "        # Convert phrase tags to indices\n",
    "        p = [self.phrase2idx.get(tag, 0) for tag in self.phrases[idx]]\n",
    "        # Pad or trim sequences to the fixed length\n",
    "        x = x[:self.sequence_len] + [0]*(self.sequence_len - len(x))\n",
    "        y = y[:self.sequence_len] + [0]*(self.sequence_len - len(y))\n",
    "        p = p[:self.sequence_len] + [0]*(self.sequence_len - len(p))\n",
    "        # Return tensors for model input\n",
    "        return torch.tensor(x), torch.tensor(y), torch.tensor(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832bcf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of all unique characters in both English and Hindi sentences\n",
    "chars = set(''.join(en_proc + hi_proc))\n",
    "# Create a mapping from character to index, with <pad> as the first token\n",
    "ch2i = {c: i for i, c in enumerate(['<pad>'] + sorted(list(chars)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c84a3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset object for training\n",
    "dataset = CharPhraseDataset(en_proc, hi_proc, phrase_tags, sequence_len, ch2i, phrase2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9621be52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model and configuration classes\n",
    "from transformer import TransformerWithPhrase\n",
    "from transformer import TransformerConfig\n",
    "\n",
    "# Set up the model configuration (adjust parameters as needed)\n",
    "mconfig = TransformerConfig(\n",
    "    vocab_size=len(ch2i),           # Number of unique characters\n",
    "    sequence_len=sequence_len,      # Maximum sequence length\n",
    "    nblock=4,                       # Number of transformer blocks\n",
    "    nhead=8,                        # Number of attention heads\n",
    "    embed_dim=256,                  # Embedding dimension\n",
    "    phrase_emb_dim=16,              # Phrase embedding dimension\n",
    ")\n",
    "# Create the transformer model\n",
    "model = TransformerWithPhrase(mconfig, phrase_vocab_size=len(phrase2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c6815a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.7379\n",
      "Epoch 2, Loss: 3.0042\n",
      "Epoch 3, Loss: 2.7939\n",
      "Epoch 3, Loss: 2.7939\n",
      "Epoch 4, Loss: 2.6110\n",
      "Epoch 4, Loss: 2.6110\n",
      "Epoch 5, Loss: 2.3312\n",
      "Epoch 6, Loss: 1.8875\n",
      "Epoch 5, Loss: 2.3312\n",
      "Epoch 6, Loss: 1.8875\n",
      "Epoch 7, Loss: 1.2686\n",
      "Epoch 8, Loss: 0.7959\n",
      "Epoch 7, Loss: 1.2686\n",
      "Epoch 8, Loss: 0.7959\n",
      "Epoch 9, Loss: 0.4958\n",
      "Epoch 10, Loss: 0.3126\n",
      "Epoch 9, Loss: 0.4958\n",
      "Epoch 10, Loss: 0.3126\n"
     ]
    }
   ],
   "source": [
    "# Import the training classes\n",
    "from trainer import Trainer, TrainerConfig\n",
    "# Set up training configuration (adjust parameters as needed)\n",
    "trainer_config = TrainerConfig(max_epochs=10, batch_size=64, learning_rate=3e-4, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Create the trainer object\n",
    "trainer = Trainer(model, dataset, trainer_config)\n",
    "# Start the training process\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f8d793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "दुनिे दुनिया।या।ुनिया।ुनिया।प \n",
      "दुनaसे हैं?ा।प दुनिया।aदुनिया\n"
     ]
    }
   ],
   "source": [
    "# Test the model with new English sentences and print Hindi translations\n",
    "from preprocessing import extract_7_phrases\n",
    "\n",
    "# List of English sentences to translate\n",
    "test_sents = [\"This is a test.\", \"Translate this sentence.\"]\n",
    "\n",
    "# Extract phrase tags for each test sentence\n",
    "test_phrases = [extract_7_phrases(s) for s in test_sents]\n",
    "\n",
    "# Convert phrase tags and characters to indices, pad to sequence length\n",
    "test_p = [[phrase2idx.get(tag, 0) for tag in tags] + [0]*(sequence_len-len(tags)) for tags in test_phrases]\n",
    "test_x = [[ch2i.get(c, 0) for c in s] + [0]*(sequence_len-len(s)) for s in test_sents]\n",
    "\n",
    "# Move tensors to the correct device (CPU or GPU)\n",
    "device = trainer_config.device\n",
    "test_x = torch.tensor(test_x).to(device)\n",
    "test_p = torch.tensor(test_p).to(device)\n",
    "\n",
    "# Generate translations using the trained model\n",
    "with torch.no_grad():\n",
    "    translations = model.generate(test_x, test_p)\n",
    "\n",
    "# Convert output indices back to text\n",
    "i2ch = {i: c for c, i in ch2i.items()}\n",
    "\n",
    "def decode(indices):\n",
    "    # Convert a list of indices to a string, ignoring padding\n",
    "    return ''.join([i2ch.get(idx, '') for idx in indices if idx != 0])\n",
    "\n",
    "# Print the translated Hindi sentences\n",
    "for sent in translations.cpu().numpy():\n",
    "    print(decode(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e29c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
